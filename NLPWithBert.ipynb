{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqbHS9ZcSL0lGfwjxT5UG1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3bbinesh/ML_NLP/blob/main/NLPWithBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MPfIJ2Qjsr74"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, BertModel\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "#sentence1 = sys.argv[1]  # First sentence\n",
        "#sentence2 = sys.argv[2]  # Second sentence\n",
        "sentence1 = \"The city of joy\"\n",
        "sentence2 = \"better known as Kolkata\"\n",
        "sentences = [sentence1, sentence2]"
      ],
      "metadata": {
        "id": "tKoLjPDJtD30"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "sentences_tokenized = [tokenizer.tokenize(s) for s in sentences]\n",
        "MAX_LENGTH = max([len(sentences_tokenized[0]), len(sentences_tokenized[1])])"
      ],
      "metadata": {
        "id": "3vzshkaSuHC4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate IDs of each token and add padding to sentences smaller than given threshold\n",
        "ids = [tokenizer.convert_tokens_to_ids(t) for t in sentences_tokenized]\n",
        "# Pad the tokens\n",
        "ids = np.asarray([np.pad(i, (0, MAX_LENGTH-len(i)), mode='constant') for i in ids])"
      ],
      "metadata": {
        "id": "E5zMegFVuSF0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the masks\n",
        "amasks = np.asarray([[float(i>0) for i in seq] for seq in ids])\n",
        "\n",
        "# Get the output from the model\n",
        "output = model.forward(torch.tensor(ids), torch.tensor(amasks))\n",
        "\n",
        "# Now create two vector representations for each sentence\n",
        "# pool_vectors (2x768) is the vector generated by perofrming max-pooling over the hidden states\n",
        "pool_vectors = torch.max(output.last_hidden_state, dim=1)[0]\n",
        "# cls_vectors (2x768) is the vector generated by taking the CLS token\n",
        "cls_vectors = output.last_hidden_state[:, 0, :]"
      ],
      "metadata": {
        "id": "dEloTX2JuYz8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = torch.dot(vec1, vec2)\n",
        "    norm1 = torch.norm(vec1)\n",
        "    norm2 = torch.norm(vec2)\n",
        "    cosine_sim = dot_product / (norm1 * norm2)\n",
        "    return cosine_sim"
      ],
      "metadata": {
        "id": "YPdQModWukSe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cosine_pooling = --------------       # Fill up this '----------' section\n",
        "cosine_pooling = cosine_similarity(pool_vectors[0], pool_vectors[1])\n",
        "#cosine_cls = ------------------       # Fill up this '----------' section\n",
        "cosine_cls = cosine_similarity(cls_vectors[0], cls_vectors[1])\n",
        "\n",
        "# Finally print out the values\n",
        "print(np.round(cosine_pooling.item(), 2), np.round(cosine_cls.item(), 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXCQYxipusxH",
        "outputId": "c01e407f-202a-4153-84fa-ed5c90c4a9d3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.51 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Tokenize each sentence\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "    sentences_tokenized = [tokenizer.tokenize(s) for s in sentences]\n",
        "    MAX_LENGTH = max([len(sentences_tokenized[0]), len(sentences_tokenized[1])])\n",
        "\n",
        "    # Generate IDs of each token and add padding to sentences smaller than given threshold\n",
        "    ids = [tokenizer.convert_tokens_to_ids(t) for t in sentences_tokenized]\n",
        "    # Pad the tokens\n",
        "    ids = np.asarray([np.pad(i, (0, MAX_LENGTH-len(i)), mode='constant') for i in ids])\n",
        "\n",
        "\n",
        "    # Generate the masks\n",
        "    amasks = np.asarray([[float(i>0) for i in seq] for seq in ids])\n",
        "\n",
        "    # Get the output from the model\n",
        "    output = model.forward(torch.tensor(ids), torch.tensor(amasks))\n",
        "\n",
        "    # Now create two vector representations for each sentence\n",
        "    # pool_vectors (2x768) is the vector generated by perofrming max-pooling over the hidden states\n",
        "    pool_vectors = torch.max(output.last_hidden_state, dim=1)[0]\n",
        "    # cls_vectors (2x768) is the vector generated by taking the CLS token\n",
        "    cls_vectors = output.last_hidden_state[:, 0, :]\n",
        "\n",
        "    # Compute the cosine similarity\n",
        "    def cosine_similarity(vec1, vec2):\n",
        "        dot_product = torch.dot(vec1, vec2)\n",
        "        norm1 = torch.norm(vec1)\n",
        "        norm2 = torch.norm(vec2)\n",
        "        cosine_sim = dot_product / (norm1 * norm2)\n",
        "        return cosine_sim\n",
        "    #cosine_pooling = --------------       # Fill up this '----------' section\n",
        "    cosine_pooling = cosine_similarity(pool_vectors[0], pool_vectors[1])\n",
        "    #cosine_cls = ------------------       # Fill up this '----------' section\n",
        "    cosine_cls = cosine_similarity(cls_vectors[0], cls_vectors[1])\n",
        "\n",
        "    # Finally print out the values\n",
        "    print(np.round(cosine_pooling.item(), 2), np.round(cosine_cls.item(), 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arJ6SjW3swoB",
        "outputId": "65df047f-e86b-49a7-b81f-c6c23493fffe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.89 0.85\n"
          ]
        }
      ]
    }
  ]
}